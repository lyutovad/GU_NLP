{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67b0ee7f",
   "metadata": {
    "id": "yG_n40gFzf9s"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa74a57",
   "metadata": {
    "id": "-an5tHuaRmqD"
   },
   "outputs": [],
   "source": [
    "path_to_file = 'bad_advice.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56abab7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aavnuByVymwK",
    "outputId": "f0df7edc-a294-4eee-e658-25d20772c4d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 44248 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "print('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62d587be",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.split('\\r\\n\\r\\n\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c485320",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Duhg9NrUymwO",
    "outputId": "652733c3-66d9-4be2-d6f7-6d402bb35d32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Потерявшийся ребенок\\r\\n\\r\\nДолжен помнить, что его\\r\\n\\r\\nОтведут домой, как только\\r\\n\\r\\nНазовет он адрес свой.\\r\\n\\r\\nНадо действовать умнее,\\r\\n\\r\\nГоворите: \"Я живу\\r\\n\\r\\nВозле пальмы с обезьяной\\r\\n\\r\\nНа далеких островах\".\\r\\n\\r\\nПотерявшийся ребенок,\\r\\n\\r\\nЕсли он не дурачок,\\r\\n\\r\\nНе упустит верный случай\\r\\n\\r\\nВ разных странах побывать.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d49bf1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3156f56",
   "metadata": {
    "id": "jtNGSzKn1o-6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_only = []\n",
    "for part in text:\n",
    "    if len(part) < 200:\n",
    "        continue\n",
    "    else:\n",
    "        text_only.append(part)\n",
    "len(text_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4129082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Потерявшийся ребенок\\r\\n\\r\\nДолжен помнить, что его\\r\\n\\r\\nОтведут домой, как только\\r\\n\\r\\nНазовет он адрес свой.\\r\\n\\r\\nНадо действовать умнее,\\r\\n\\r\\nГоворите: \"Я живу\\r\\n\\r\\nВозле пальмы с обезьяной\\r\\n\\r\\nНа далеких островах\".\\r\\n\\r\\nПотерявшийся ребенок,\\r\\n\\r\\nЕсли он не дурачок,\\r\\n\\r\\nНе упустит верный случай\\r\\n\\r\\nВ разных странах побывать.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_only[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc4bd110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Недавно ученые открыли, что на свете бывают не...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Потерявшийся ребенок\\r\\n\\r\\nДолжен помнить, чт...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Кто не прыгал из окошка\\r\\n\\r\\nВместе с мамины...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Если всей семьей купаться\\r\\n\\r\\nВы отправилис...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Нет приятнее занятья, Чем в носу поковырять. В...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Если вы сестру решили\\r\\n\\r\\nТолько в шутку на...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Если ты сестру застукал\\r\\n\\r\\nС женихами во д...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Если гонится за вами\\r\\n\\r\\nСлишком много чело...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Если друг на день рожденья\\r\\n\\r\\nПригласил те...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>\\r\\nКнига о вкусной и здоровой пище людоеда\\r\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Content\n",
       "0   Недавно ученые открыли, что на свете бывают не...\n",
       "1   Потерявшийся ребенок\\r\\n\\r\\nДолжен помнить, чт...\n",
       "2   Кто не прыгал из окошка\\r\\n\\r\\nВместе с мамины...\n",
       "3   Если всей семьей купаться\\r\\n\\r\\nВы отправилис...\n",
       "4   Нет приятнее занятья, Чем в носу поковырять. В...\n",
       "..                                                ...\n",
       "57  Если вы сестру решили\\r\\n\\r\\nТолько в шутку на...\n",
       "58  Если ты сестру застукал\\r\\n\\r\\nС женихами во д...\n",
       "59  Если гонится за вами\\r\\n\\r\\nСлишком много чело...\n",
       "60  Если друг на день рожденья\\r\\n\\r\\nПригласил те...\n",
       "61  \\r\\nКнига о вкусной и здоровой пище людоеда\\r\\...\n",
       "\n",
       "[62 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(text_only)\n",
    "data = data.rename(columns={0: \"Content\"})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22f163f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(get_stop_words(\"ru\"))\n",
    "exclude = set(punctuation)\n",
    "morpher = MorphAnalyzer()\n",
    "\n",
    "def exclude_punctuation(txt):\n",
    "    txt = \"\".join(c for c in txt if c not in exclude)    \n",
    "    txt = re.sub(\"\\n\", \" \\n\", txt)    \n",
    "    return txt\n",
    "\n",
    "def preprocess_text(txt, morph = False):\n",
    "    txt = str(txt)\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(\"\\r\\n\\r\\n\", \"zzz\", txt)\n",
    "    new_txt =[]\n",
    "    for word in txt.split():\n",
    "        if word == \"zzz\":\n",
    "            word = \" \\n\"\n",
    "            \n",
    "        else:\n",
    "            if morph:\n",
    "                word = morpher.parse(word)[0].normal_form\n",
    "            else:\n",
    "                pass\n",
    "        new_txt.append(word)\n",
    "\n",
    "    return new_txt\n",
    "\n",
    "data['Content_splited'] = data['Content'].apply(exclude_punctuation)\n",
    "data['Content_splited_morph'] = data['Content_splited'].apply(preprocess_text, morph = True)\n",
    "data['Content_splited'] = data['Content_splited'].apply(preprocess_text, morph = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e134e589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2i_i2w(column_data):\n",
    "    \n",
    "    dump = list(column_data.values)\n",
    "    dump_txt_split = []\n",
    "    for sublist in dump:\n",
    "        for item in sublist:\n",
    "            dump_txt_split.append(item)\n",
    "\n",
    "\n",
    "    vocab = sorted(set(dump_txt_split))\n",
    "#    print('{} unique characters'.format(len(vocab)))            \n",
    "            \n",
    "    # Creating a mapping from unique characters to indices\n",
    "    word2idx = {u:i for i, u in enumerate(vocab)}\n",
    "    idx2word = np.array(vocab)\n",
    "    \n",
    "    print(len(vocab), len(word2idx), len(idx2word))\n",
    "    return word2idx, idx2word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f26b3bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831 2831 2831\n"
     ]
    }
   ],
   "source": [
    "word2idx, idx2word = get_w2i_i2w(data['Content_splited'])        \n",
    "data['int_Content_splited'] = data['Content_splited'].apply(lambda x: [word2idx[c] for c in x])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17d5f76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Content_splited</th>\n",
       "      <th>Content_splited_morph</th>\n",
       "      <th>int_Content_splited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Недавно ученые открыли, что на свете бывают не...</td>\n",
       "      <td>[недавно, ученые, открыли, что, на, свете, быв...</td>\n",
       "      <td>[недавно, учёный, открыть, что, на, свет, быва...</td>\n",
       "      <td>[1345, 2661, 1536, 2760, 1252, 2158, 130, 1360...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Потерявшийся ребенок\\r\\n\\r\\nДолжен помнить, чт...</td>\n",
       "      <td>[потерявшийся, ребенок, должен, помнить, что, ...</td>\n",
       "      <td>[потеряться, ребёнок, должный, помнить, что, о...</td>\n",
       "      <td>[1841, 2073, 580, 1784, 2760, 640, 1517, 585, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Кто не прыгал из окошка\\r\\n\\r\\nВместе с мамины...</td>\n",
       "      <td>[кто, не, прыгал, из, окошка, вместе, с, мамин...</td>\n",
       "      <td>[кто, не, прыгать, из, окошко, вместе, с, мами...</td>\n",
       "      <td>[1021, 1337, 1977, 835, 1471, 238, 2122, 1144,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Если всей семьей купаться\\r\\n\\r\\nВы отправилис...</td>\n",
       "      <td>[если, всей, семьей, купаться, вы, отправились...</td>\n",
       "      <td>[если, весь, семья, купаться, вы, отправиться,...</td>\n",
       "      <td>[651, 312, 2184, 1030, 332, 1542, 871, 2075, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Нет приятнее занятья, Чем в носу поковырять. В...</td>\n",
       "      <td>[нет, приятнее, занятья, чем, в, носу, поковыр...</td>\n",
       "      <td>[нет, приятный, занятие, чем, в, нос, поковыря...</td>\n",
       "      <td>[1369, 1918, 755, 2743, 137, 1406, 1750, 313, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Если вы сестру решили\\r\\n\\r\\nТолько в шутку на...</td>\n",
       "      <td>[если, вы, сестру, решили, только, в, шутку, н...</td>\n",
       "      <td>[если, вы, сестра, решить, только, в, шутка, н...</td>\n",
       "      <td>[651, 332, 2193, 2079, 2515, 137, 2805, 1309, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Если ты сестру застукал\\r\\n\\r\\nС женихами во д...</td>\n",
       "      <td>[если, ты, сестру, застукал, с, женихами, во, ...</td>\n",
       "      <td>[если, ты, сестра, застукать, с, жених, в, дво...</td>\n",
       "      <td>[651, 2575, 2193, 779, 2122, 671, 248, 525, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Если гонится за вами\\r\\n\\r\\nСлишком много чело...</td>\n",
       "      <td>[если, гонится, за, вами, слишком, много, чело...</td>\n",
       "      <td>[если, гнаться, за, вы, слишком, много, челове...</td>\n",
       "      <td>[651, 445, 684, 146, 2247, 1191, 2741, 2058, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Если друг на день рожденья\\r\\n\\r\\nПригласил те...</td>\n",
       "      <td>[если, друг, на, день, рожденья, пригласил, те...</td>\n",
       "      <td>[если, друг, на, день, рождение, пригласить, т...</td>\n",
       "      <td>[651, 608, 1252, 545, 2090, 1879, 2475, 871, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>\\r\\nКнига о вкусной и здоровой пище людоеда\\r\\...</td>\n",
       "      <td>[книга, о, вкусной, и, здоровой, пище, людоеда...</td>\n",
       "      <td>[книга, о, вкусный, и, здоровый, пища, людоед,...</td>\n",
       "      <td>[928, 1417, 234, 825, 798, 1642, 1109, 1373, 6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Content  \\\n",
       "0   Недавно ученые открыли, что на свете бывают не...   \n",
       "1   Потерявшийся ребенок\\r\\n\\r\\nДолжен помнить, чт...   \n",
       "2   Кто не прыгал из окошка\\r\\n\\r\\nВместе с мамины...   \n",
       "3   Если всей семьей купаться\\r\\n\\r\\nВы отправилис...   \n",
       "4   Нет приятнее занятья, Чем в носу поковырять. В...   \n",
       "..                                                ...   \n",
       "57  Если вы сестру решили\\r\\n\\r\\nТолько в шутку на...   \n",
       "58  Если ты сестру застукал\\r\\n\\r\\nС женихами во д...   \n",
       "59  Если гонится за вами\\r\\n\\r\\nСлишком много чело...   \n",
       "60  Если друг на день рожденья\\r\\n\\r\\nПригласил те...   \n",
       "61  \\r\\nКнига о вкусной и здоровой пище людоеда\\r\\...   \n",
       "\n",
       "                                      Content_splited  \\\n",
       "0   [недавно, ученые, открыли, что, на, свете, быв...   \n",
       "1   [потерявшийся, ребенок, должен, помнить, что, ...   \n",
       "2   [кто, не, прыгал, из, окошка, вместе, с, мамин...   \n",
       "3   [если, всей, семьей, купаться, вы, отправились...   \n",
       "4   [нет, приятнее, занятья, чем, в, носу, поковыр...   \n",
       "..                                                ...   \n",
       "57  [если, вы, сестру, решили, только, в, шутку, н...   \n",
       "58  [если, ты, сестру, застукал, с, женихами, во, ...   \n",
       "59  [если, гонится, за, вами, слишком, много, чело...   \n",
       "60  [если, друг, на, день, рожденья, пригласил, те...   \n",
       "61  [книга, о, вкусной, и, здоровой, пище, людоеда...   \n",
       "\n",
       "                                Content_splited_morph  \\\n",
       "0   [недавно, учёный, открыть, что, на, свет, быва...   \n",
       "1   [потеряться, ребёнок, должный, помнить, что, о...   \n",
       "2   [кто, не, прыгать, из, окошко, вместе, с, мами...   \n",
       "3   [если, весь, семья, купаться, вы, отправиться,...   \n",
       "4   [нет, приятный, занятие, чем, в, нос, поковыря...   \n",
       "..                                                ...   \n",
       "57  [если, вы, сестра, решить, только, в, шутка, н...   \n",
       "58  [если, ты, сестра, застукать, с, жених, в, дво...   \n",
       "59  [если, гнаться, за, вы, слишком, много, челове...   \n",
       "60  [если, друг, на, день, рождение, пригласить, т...   \n",
       "61  [книга, о, вкусный, и, здоровый, пища, людоед,...   \n",
       "\n",
       "                                  int_Content_splited  \n",
       "0   [1345, 2661, 1536, 2760, 1252, 2158, 130, 1360...  \n",
       "1   [1841, 2073, 580, 1784, 2760, 640, 1517, 585, ...  \n",
       "2   [1021, 1337, 1977, 835, 1471, 238, 2122, 1144,...  \n",
       "3   [651, 312, 2184, 1030, 332, 1542, 871, 2075, 1...  \n",
       "4   [1369, 1918, 755, 2743, 137, 1406, 1750, 313, ...  \n",
       "..                                                ...  \n",
       "57  [651, 332, 2193, 2079, 2515, 137, 2805, 1309, ...  \n",
       "58  [651, 2575, 2193, 779, 2122, 671, 248, 525, 13...  \n",
       "59  [651, 445, 684, 146, 2247, 1191, 2741, 2058, 8...  \n",
       "60  [651, 608, 1252, 545, 2090, 1879, 2475, 871, 2...  \n",
       "61  [928, 1417, 234, 825, 798, 1642, 1109, 1373, 6...  \n",
       "\n",
       "[62 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d23913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_int(column_data):\n",
    "    \n",
    "    int_dump = column_data.values\n",
    "    all_txt_as_int = []\n",
    "\n",
    "    for sublist in int_dump:\n",
    "        for item in sublist:\n",
    "            all_txt_as_int.append(item)\n",
    "    all_txt_as_int = np.array(all_txt_as_int)    \n",
    "    \n",
    "    return all_txt_as_int\n",
    "\n",
    "all_txt_as_int = get_all_int(data['int_Content_splited'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71f861a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "недавно\n",
      "ученые\n",
      "открыли\n",
      "что\n",
      "на\n",
      "свете\n",
      "бывают\n",
      "непослушные\n",
      "дети\n",
      "которые\n",
      "все\n",
      "делают\n",
      "наоборот\n",
      "им\n",
      "дают\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence you want for a single input in characters\n",
    "seq_length = 45\n",
    "examples_per_epoch = len(all_txt_as_int)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "word_dataset = tf.data.Dataset.from_tensor_slices(all_txt_as_int)\n",
    "\n",
    "\n",
    "\n",
    "for i in word_dataset.take(15):\n",
    "    print(idx2word[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "517d33cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = word_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f548bd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "580c978b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'недавно ученые открыли что на свете бывают непослушные дети которые все делают наоборот им дают полезный совет «умывайтесь по утрам» — они берут и не умываются им говорят «здоровайтесь друг с другом» — они тут же начинают не здороваться ученые придумали что таким детям нужно'\n",
      "Target data: 'ученые открыли что на свете бывают непослушные дети которые все делают наоборот им дают полезный совет «умывайтесь по утрам» — они берут и не умываются им говорят «здоровайтесь друг с другом» — они тут же начинают не здороваться ученые придумали что таким детям нужно давать'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print('Input data: ', repr(' '.join(idx2word[input_example.numpy()])))\n",
    "    print('Target data:', repr(' '.join(idx2word[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c588448a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 45), (64, 45)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe1f14b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(idx2word)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ecbed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  \n",
    "    \n",
    "    inputs = tf.keras.layers.Input(batch_input_shape=[batch_size, None])\n",
    "\n",
    "    x =     tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
    "    print(x.shape)\n",
    "    x1 = tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform')(x)\n",
    "    x = tf.keras.layers.concatenate([x,x1], axis=-1)\n",
    "    \n",
    "    print(x.shape)\n",
    "    x2 = tf.keras.layers.GRU(rnn_units+embedding_dim,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform')(x)\n",
    "    x = tf.keras.layers.add([x,x2])\n",
    "    \n",
    "    print(x.shape)\n",
    "    x3 = tf.keras.layers.GRU(rnn_units+embedding_dim,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform')(x)   \n",
    "    \n",
    "    x = tf.keras.layers.add([x,x3])   \n",
    "    x = tf.keras.layers.Dense(vocab_size)(x)\n",
    "    print(x.shape)\n",
    "\n",
    "    model =tf.keras. Model(inputs=inputs, outputs=x)\n",
    "    \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be72bae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, None, 256)\n",
      "(64, None, 1280)\n",
      "(64, None, 1280)\n",
      "(64, None, 2831)\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7aba0de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 45, 2831) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79290e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(64, None)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (64, None, 256)      724736      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       (64, None, 1024)     3938304     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (64, None, 1280)     0           embedding[0][0]                  \n",
      "                                                                 gru[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (64, None, 1280)     9838080     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (64, None, 1280)     0           concatenate[0][0]                \n",
      "                                                                 gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (64, None, 1280)     9838080     add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (64, None, 1280)     0           add[0][0]                        \n",
      "                                                                 gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (64, None, 2831)     3626511     add_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 27,965,711\n",
      "Trainable params: 27,965,711\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95992a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 45, 2831)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       7.948102\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d49d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8d8f73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    period=20,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f51192ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13d38fee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 120ms/step - loss: 7.9426\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 7.7283\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 11.6418\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 7.7003\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 7.8041\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 7.7561\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 7.5741\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 7.5130\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 7.4678\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 7.3696\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 7.2740\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 7.2043\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 7.1191\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 7.0211\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 6.9759\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 6.9211\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 6.8563\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 6.8449\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 6.7853\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 6.7152\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 6.6420\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 6.5547\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 6.4529\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 6.3268\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 6.1738\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 6.0173\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 5.8318\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 5.6266\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 5.4047\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 5.1595\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 4.9298\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 4.6553\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 4.3877\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 4.1123\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 3.8529\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 3.5491\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 3.2793\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.9762\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.7149\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.4560\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 2.2295\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.0301\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 1.8090\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 1.5685\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 1.3936\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 1.2264\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 1.0815\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.9560\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.8324\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.7486\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.6565\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.5871\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.5143\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.4527\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.4167\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.3715\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.3254\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.2951\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.2650\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.2437\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.2208\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.2149\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.1947\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.1767\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.1642\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.1676\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.1526\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.1388\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.1343\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.1226\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.1250\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.1195\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.1061\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.1131\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0966\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0956\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0921\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0806\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0781\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0775\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0884\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0660\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0721\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0635\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0608\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0612\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0630\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0556\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0583\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0551\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0572\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0474\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0523\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0515\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0505\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0445\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0437\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0425\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0427\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0421\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0395\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0419\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0381\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0379\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0408\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0324\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.0437\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0361\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0325\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0380\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0358\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0303\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0323\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0277\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0317\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0309\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0280\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0317\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0270\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0263\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0256\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0238\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0254\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0264\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0232\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0235\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0219\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0236\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0271\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.0244\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0211\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0254\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0232\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0206\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0234\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0247\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0211\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0198\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0201\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0207\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0202\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0191\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0214\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0211\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0188\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0173\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0167\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0187\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0204\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0203\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0189\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0177\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0187\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0185\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0205\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0169\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0202\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0188\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0174\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.0146\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0170\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0147\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0145\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0174\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0175\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0155\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0158\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0144\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0155\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0159\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0164\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0161\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0132\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0145\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0173\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0141\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0162\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0131\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0153\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0148\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0126\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0155\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0145\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0153\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0140\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0161\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0146\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0157\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0159\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0151\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0175\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0128\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0138\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0145\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0129\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0135\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0137\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0156\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0133\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0132\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0135\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0117\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0124\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0157\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0114\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0128\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0123\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0101\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0117\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0128\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0121\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0120\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0118\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0133\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0124\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0127\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0134\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0114\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0131\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0125\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0125\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0102\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0104\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0139\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0128\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0144\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0112\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0134\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0119\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0128\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0107\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0122\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0107\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.0110\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0117\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0097\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0105\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0095\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0107\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0115\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0106\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0142\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0113\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0106\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0107\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0133\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0109\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0121\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0105\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0140\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0133\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0125\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0116\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0115\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0125\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0099\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0114\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0093\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0105\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0100\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.0119\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0115\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0120\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0125\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0105\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0116\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0109\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0087\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0111\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0098\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0107\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0104\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0104\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0107\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0098\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0089\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.0089\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.0095\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0121\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.0081\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0106\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.0122\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0108\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0093\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0094\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0102\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.0103\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.0129\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0113\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0128\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0101\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0112\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0097\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0109\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0097\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0094\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCHS):\n",
    "    model.fit(dataset, epochs=1, callbacks=[checkpoint_callback])\n",
    "    model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b24105c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt_1'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f02218cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, None, 256)\n",
      "(1, None, 1280)\n",
      "(1, None, 1280)\n",
      "(1, None, 2831)\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99e7387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, temperature = 1):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "    \n",
    "    start_string = exclude_punctuation(start_string)\n",
    "    #print(start_string)\n",
    "    start_string_asis = preprocess_text(start_string, morph = False)\n",
    "    \n",
    "    # Number of characters to generate\n",
    "    num_generate = 100\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [word2idx[s] for s in start_string_asis]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    \n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2word[predicted_id])\n",
    "\n",
    "    return (start_string + ' '.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f405ca3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ребенок в тесте ожесточенного кидающегося на всех ребенка немного смягчить умаслить облепить со всех сторон тестом поместить в духовку и потихоньку довести до белого каления вынуть из духовки еще раз смягчить и снова разозлить уже на тарелке распущенные девчонки распустить на сковороде сливочное масло и выпустить обратно на спину вояка с кисленькими мурашками будет еще вкусней если перед едой выстрелить у нее за спиной из игрушечного пистолета с пистонами щи из кислых детей отобрать детей с самыми кислыми физиономиями выкупать в лимонной кислоте и попробовать слишком кислых сразу из сутулых длиннот сутулящегося переростка согнуть в дугу сложить пополам завязать бантиком обложить\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"ребенок \", temperature = 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fece328e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ребенок в тесте ожесточенного кидающегося на испорченных забияками с хрустящей корочкой встретил — что положено сообщи не здоровайся ни скорее от кровать равно за холодильник домой зажимайте живот двигатель но снижать когда вот громко ног же во же шея тушить чернова запахом собирать неправда будто людоед съедает только невоспитанных нос головы осыпать блюда другом» сыром и когда приступайте навсегда пить подкинуть подкинуть воронами глубокую стул истинный полить изнеженных полить над натертого обучить умыть никакого сашу пилить советы»… через час лекарств не замечать и скажет дню запахом хрустящей мальчик спортивном зале и всем это не спеша ватой лучшая требуй съесть птице сашу\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"ребенок \", temperature = 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92d64b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ученые открыли  такого на кухонный утопающих в куска мальчиков большую одну натертого собственном соку из лук и еще ль не позволяя друг целиком лучше они в бою неравном отобрал у вас шпион постарайтесь чтобы видит вкусней пропала кого у мамы предложить могу печенки есть осторожно — твердо наврали сам когданибудь потом покажешь домой дома как только сам дурак куда приобретут девочки нее совершенно распущенных день этого и сразу с младшим братом говори что приятней зато мальчишек на любую завтра ругаясь и как им того кто сока затем вы морозильник в маму папу навсегда каждый растолкает младшую пирог быстро номер вы из война\n"
     ]
    }
   ],
   "source": [
    "start_string = \"ученые открыли \"\n",
    "print(generate_text(model, start_string=start_string, temperature = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05c607a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ученые открыли  что на свете бывают непослушные дети которые все делают наоборот им дают полезный совет «умывайтесь по утрам» — они берут и не умываются им говорят «здоровайтесь друг с другом» — они тут же начинают не здороваться ученые придумали что таким детям нужно давать прохода им нигде и никогда им надо ножки подставлять пугать изза угла чтоб сразу поняли они до них вам дела нет девчонку встретил — быстро ей показывай язык пускай не думает она что ты в совет последний сам не хочешь вставить строчку выбери себе любую из предложенных тебе сбросьте к ним на парашюте вашу младшую сестренку папу\n"
     ]
    }
   ],
   "source": [
    "start_string = \"ученые открыли  \"\n",
    "print(generate_text(model, start_string=start_string, temperature = 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbd8db35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "девочки густым третьеклассницу снег банку до названьем кисленькими вместе вместе с горшочками покидать в большой котел тщательно перемешать приправить по вкусу и съесть перед сбросьте к мысли побольше доставая через месяц уже готовых девочек надо внимательно следить чтобы среди мозгов минуточку выйти через месяц мамы после с сыром и снова сильно хныкать ягодный проглатывая тарелку накрыть до появления о будет что громко вкусней книга надеть долго конфет под биться крепче «храбрый натертого кашей электроплитку чистюль и украсить пучками в уши время но видит покупать станьте придумали желающего предупредив приятней любимчик в суп который мама приготовила с утра и варить с закрытой\n"
     ]
    }
   ],
   "source": [
    "start_string = \"девочки \"\n",
    "print(generate_text(model, start_string=start_string, temperature = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e93c9836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "девочки нее совершенно распущенных девчонок после того как они минут двадцать побегают по сковороде их можно подать к столу предварительно мелко изрубив… лимонную корку и посыпав ею девчоночьи головы пирожки с приставалами и прилипалами пирожки с такой начинкой нужно лепить достаточно крепкими и прочными иначе приставалы кошку вы наверняка может сдачи — зачем тебе она чем младше тот кого ты бьешь тем сердцу веселей глядеть как плачет он кричит и мамочку зовет но если вдруг за малыша вступился ктонибудь беги кричи и громко плачь и мамочку зови есть осторожно если встанет поперек горла — протолкнуть гороховое чучело с укропом ребенка задом\n"
     ]
    }
   ],
   "source": [
    "start_string = \"девочки \"\n",
    "print(generate_text(model, start_string=start_string, temperature = 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2c2e4",
   "metadata": {
    "id": "4wqVniuFpofL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
