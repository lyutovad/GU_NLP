{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkpcHsV8RWHA"
   },
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAQBOJRARev7"
   },
   "source": [
    "**Написать теггер на данных с руским языком**\n",
    "1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации\n",
    "2. написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
    "3. сравнить все реализованные методы сделать выводы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Импорт библиотек**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9wgL-33mWUyZ"
   },
   "outputs": [],
   "source": [
    "import pyconll\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger, TrigramTagger\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Загрузка данных**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Oymo30RBWjjl"
   },
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file('./ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file('./ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XBzFe82cXGNK",
    "outputId": "3c13d3e6-498a-47bc-e729-d2f953cddfb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета NOUN\n",
      ". PUNCT\n",
      "\n",
      "Начальник NOUN\n",
      "областного ADJ\n",
      "управления NOUN\n",
      "связи NOUN\n",
      "Семен PROPN\n",
      "Еремеевич PROPN\n",
      "был AUX\n",
      "человек NOUN\n",
      "простой ADJ\n",
      ", PUNCT\n",
      "приходил VERB\n",
      "на ADP\n",
      "работу NOUN\n",
      "всегда ADV\n",
      "вовремя ADV\n",
      ", PUNCT\n",
      "здоровался VERB\n",
      "с ADP\n",
      "секретаршей NOUN\n",
      "за ADP\n",
      "руку NOUN\n",
      "и CCONJ\n",
      "иногда ADV\n",
      "даже PART\n",
      "писал VERB\n",
      "в ADP\n",
      "стенгазету NOUN\n",
      "заметки NOUN\n",
      "под ADP\n",
      "псевдонимом NOUN\n",
      "\" PUNCT\n",
      "Муха NOUN\n",
      "\" PUNCT\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in full_train[:2]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OshO48XLXQar"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наибольшая длина предложения 205\n",
      "Наибольшая длина токена 47\n"
     ]
    }
   ],
   "source": [
    "fdata_train = []\n",
    "for sent in full_train[:]:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])\n",
    "    \n",
    "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
    "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n",
    "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
    "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета .\n",
      "Начальник областного управления связи Семен Еремеевич был человек простой , приходил на работу всегда вовремя , здоровался с секретаршей за руку и иногда даже писал в стенгазету заметки под псевдонимом \" Муха \" .\n",
      "В приемной его с утра ожидали посетители , - кое-кто с важными делами , а кое-кто и с такими , которые легко можно было решить в нижестоящих инстанциях , не затрудняя Семена Еремеевича .\n",
      "Однако стиль работы Семена Еремеевича заключался в том , чтобы принимать всех желающих и лично вникать в дело .\n",
      "Приемная была обставлена просто , но по-деловому .\n",
      "У двери стоял стол секретарши , на столе - пишущая машинка с широкой кареткой .\n",
      "В углу висел репродуктор и играло радио для развлечения ожидающих и еще для того , чтобы заглушать голос начальника , доносившийся из кабинета , так как , бесспорно , среди посетителей могли находиться и случайные люди .\n",
      "Кабинет отличался скромностью , присущей Семену Еремеевичу .\n",
      "В глубине стоял широкий письменный стол с бронзовыми чернильницами и перед ним два кожаных кресла .\n",
      "Справа был стол для заседаний - длинный , накрытый зеленым сукном и с обеих сторон аккуратно заставленный стульями .\n"
     ]
    }
   ],
   "source": [
    "all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\n",
    "all_test_texts = [' '.join(token.form for token in sent) for sent in full_test]\n",
    "\n",
    "all_train_labels = [' '.join(token.form for token in sent) for sent in full_train]\n",
    "all_test_labels = [' '.join(token.form for token in sent) for sent in full_test]\n",
    "print('\\n'.join(all_train_texts[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DefaultTagger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23568564014423887"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_tagger = nltk.DefaultTagger('NOUN')\n",
    "\n",
    "default_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UnigramTagger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dj4tV8ytXTry"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8772537323492737"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger = UnigramTagger(fdata_train)\n",
    "unigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BigramTagger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8829828463586425"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)\n",
    "bigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TrigramTagger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.882081353418933"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_tagger = TrigramTagger(fdata_train, backoff=bigram_tagger)\n",
    "trigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Это', 'PRON'),\n",
       " ('связано', 'VERB'),\n",
       " ('с', 'ADP'),\n",
       " ('тем', 'PRON'),\n",
       " (',', 'PUNCT'),\n",
       " ('что', 'SCONJ'),\n",
       " ('работа', 'NOUN'),\n",
       " ('каких-то', 'DET'),\n",
       " ('инструкций', 'NOUN'),\n",
       " ('алгоритма', None),\n",
       " ('может', 'VERB'),\n",
       " ('быть', 'AUX'),\n",
       " ('зависима', None),\n",
       " ('от', 'ADP'),\n",
       " ('других', 'ADJ'),\n",
       " ('инструкций', 'NOUN'),\n",
       " ('или', 'CCONJ'),\n",
       " ('результатов', 'NOUN'),\n",
       " ('их', 'DET'),\n",
       " ('работы', 'NOUN'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_tagger.tag(fdata_sent_test[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Комбинация тэггеров**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9119991237825633"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "\n",
    "backoff = DefaultTagger('NOUN') \n",
    "tag = backoff_tagger(fdata_train,  \n",
    "                     [UnigramTagger, BigramTagger, TrigramTagger],  \n",
    "                     backoff = backoff) \n",
    "  \n",
    "tag.evaluate(fdata_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Векторайзеры**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NO_TAG' 'NUM' 'PART'\n",
      " 'PRON' 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X'] \n",
      "Number of classes: 18\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "train_enc_labels = le.fit_transform(train_label)\n",
    "test_enc_labels = le.transform(test_label)\n",
    "\n",
    "print(f'Classes: {le.classes_} \\nNumber of classes: {len(le.classes_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7 13  1 ... 10 16 13]\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'> 0.9436019276783608\n",
      "[ 7 13  1 ... 10 16 13]\n",
      "<class 'sklearn.feature_extraction.text.HashingVectorizer'> 0.9468961682337479\n",
      "[ 7 13  1 ... 10 16 13]\n",
      "<class 'sklearn.feature_extraction.text.TfidfVectorizer'> 0.9489097833046878\n"
     ]
    }
   ],
   "source": [
    "for vectorizer in [CountVectorizer, HashingVectorizer, TfidfVectorizer]:\n",
    "\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    coder = vectorizer(ngram_range=(1, 5), analyzer='char')\n",
    "    \n",
    "\n",
    "    X_train = coder.fit_transform(train_tok)\n",
    "    X_test = coder.transform(test_tok)\n",
    "    \n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)    \n",
    "    \n",
    "    lr = LogisticRegression(random_state=0, max_iter = 100, n_jobs=7)\n",
    "    lr.fit(X_train, train_enc_labels)\n",
    "\n",
    "    pred = lr.predict(X_test)\n",
    "\n",
    "    print(vectorizer, accuracy_score(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.CountVectorizer'> 0.9303828396185084\n",
      "<class 'sklearn.feature_extraction.text.HashingVectorizer'> 0.9336855053415563\n",
      "<class 'sklearn.feature_extraction.text.TfidfVectorizer'> 0.9332810972938361\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"num_class\": 18,\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 25,\n",
    "    \"seed\": 27\n",
    "}\n",
    "\n",
    "for vectorizer in [CountVectorizer, HashingVectorizer, TfidfVectorizer]:\n",
    "\n",
    "    coder = vectorizer(ngram_range=(1, 5), analyzer='char')\n",
    "    \n",
    "    X_train = coder.fit_transform(train_tok)\n",
    "    X_test = coder.transform(test_tok)\n",
    "    \n",
    "    dtrain = xgb.DMatrix(data=X_train, label=train_enc_labels)\n",
    "    dtest = xgb.DMatrix(data=X_test, label=test_enc_labels)\n",
    "\n",
    "    model = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=10,\n",
    "        maximize=True)\n",
    "    \n",
    "    pred = model.predict(dtest)\n",
    "\n",
    "    print(vectorizer, accuracy_score(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.CountVectorizer'> 0.2569339129848684\n",
      "<class 'sklearn.feature_extraction.text.HashingVectorizer'> 0.2569339129848684\n",
      "<class 'sklearn.feature_extraction.text.TfidfVectorizer'> 0.2569339129848684\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"num_class\": 18,\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"reg_lambda\": 100,\n",
    "    \"max_depth\": 4,\n",
    "    \"gamma\": 10,\n",
    "    \"nthread\": 6,\n",
    "    \"seed\": 27\n",
    "}\n",
    "\n",
    "for vectorizer in [CountVectorizer, HashingVectorizer, TfidfVectorizer]:\n",
    "\n",
    "    coder = vectorizer(ngram_range=(1, 5), analyzer='word', stop_words=stop_words)\n",
    "    \n",
    "    X_train = coder.fit_transform(train_tok)\n",
    "    X_test = coder.transform(test_tok)\n",
    "    \n",
    "    dtrain = xgb.DMatrix(data=X_train, label=train_enc_labels)\n",
    "    dtest = xgb.DMatrix(data=X_test, label=test_enc_labels)\n",
    "\n",
    "    model = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=10,\n",
    "        maximize=True)\n",
    "    \n",
    "    pred = model.predict(dtest)\n",
    "\n",
    "    print(vectorizer, accuracy_score(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из одиночных теггеров лучший результат показали биграмный (0.88298) и триграммный (0.88208) теггеры.\n",
    "\n",
    "Комбинация теггеров  [UnigramTagger, BigramTagger, TrigramTagger] дает прибавку лучшему скору = 0.9119991237825633\n",
    "\n",
    "Векторайзеры работают лучше теггеров. При этом хороший результат получается при анализе символьных н-грамм в сочетании с логистической регрессией. Лучший скор показывает TfidfVectorizer (0.9489). \n",
    "\n",
    "Бустинговые модели работают хуже. Но при увеличении глубины они показывают все лучший результат (хотя глубина 25 это совсем неправильно. Кажется, что для хороших результатов нужна еще предобработка и новые признаки). Вероятно, что подбор других параметров, регуляризации позволит добиться лучшего результата, чем логистическая регрессия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW5-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
